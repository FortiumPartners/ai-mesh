# Task 6.2: AlertManager Configuration for Advanced Alerting System
# Helm Chart Specialist - Sprint 6 Observability Implementation
# Integration with advanced alerting engine for intelligent correlation

apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
  labels:
    app: alertmanager
    component: advanced-alerting
data:
  alertmanager.yml: |
    global:
      # SMTP configuration for email notifications
      smtp_smarthost: 'localhost:587'
      smtp_from: 'helm-alerts@company.com'
      smtp_auth_username: '${SMTP_USERNAME}'
      smtp_auth_password: '${SMTP_PASSWORD}'

      # Slack API URL
      slack_api_url: '${SLACK_WEBHOOK_URL}'

      # PagerDuty integration key
      pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

      # HTTP configuration
      http_config:
        follow_redirects: true
        enable_http2: true

    # Template definitions for notification formatting
    templates:
      - '/etc/alertmanager/templates/*.tmpl'

    # Advanced routing configuration with intelligent correlation
    route:
      group_by: ['alertname', 'component', 'environment']
      group_wait: 30s          # Wait for additional alerts in group
      group_interval: 5m       # Send group notifications every 5 minutes
      repeat_interval: 12h     # Repeat notifications every 12 hours
      receiver: 'default-receiver'

      routes:
        # Critical Helm Chart Specialist alerts - immediate response
        - match:
            severity: critical
            component: helm-chart-specialist
          receiver: 'helm-critical-alerts'
          group_wait: 10s
          group_interval: 2m
          repeat_interval: 15m
          routes:
            # Security incidents - highest priority
            - match:
                category: security
              receiver: 'security-incident-response'
              group_wait: 0s
              repeat_interval: 5m

            # Deployment failures - immediate escalation
            - match:
                category: deployment
              receiver: 'deployment-critical'
              group_wait: 5s
              repeat_interval: 10m

        # Warning alerts with correlation
        - match:
            severity: warning
            component: helm-chart-specialist
          receiver: 'helm-warnings'
          group_wait: 2m
          group_interval: 10m
          repeat_interval: 2h

        # Performance alerts with automated optimization
        - match:
            category: performance
          receiver: 'performance-optimization'
          group_wait: 5m
          group_interval: 15m
          repeat_interval: 4h

        # Business metrics alerts
        - match:
            category: business
          receiver: 'business-alerts'
          group_wait: 10m
          group_interval: 30m
          repeat_interval: 24h

        # Development environment alerts (reduced noise)
        - match:
            environment: development
          receiver: 'dev-alerts'
          group_wait: 10m
          group_interval: 1h
          repeat_interval: 24h

    # Inhibition rules to reduce alert noise
    inhibit_rules:
      # Inhibit warning alerts when critical alerts are firing for same component
      - source_matchers:
          - severity="critical"
        target_matchers:
          - severity="warning"
        equal: ['component', 'environment']

      # Inhibit downstream alerts when upstream dependency fails
      - source_matchers:
          - alertname="HelmChartGenerationFailed"
        target_matchers:
          - alertname="HelmDeploymentSlow"
        equal: ['environment']

      # Inhibit individual alerts when correlation group is active
      - source_matchers:
          - correlation_group!=""
        target_matchers:
          - correlation_group=""
        equal: ['component', 'environment']

      # Inhibit alerts during maintenance windows
      - source_matchers:
          - maintenance_window="true"
        target_matchers:
          - maintenance_window!="true"
        equal: ['component', 'environment']

    # Receiver configurations for multi-channel notifications
    receivers:
      # Default receiver for uncategorized alerts
      - name: 'default-receiver'
        slack_configs:
          - api_url: '${SLACK_WEBHOOK_URL}'
            channel: '#helm-general'
            title: 'Helm Chart Alert'
            text: |
              {{ range .Alerts }}
              *Alert:* {{ .Annotations.summary }}
              *Severity:* {{ .Labels.severity }}
              *Component:* {{ .Labels.component }}
              *Environment:* {{ .Labels.environment }}
              *Description:* {{ .Annotations.description }}
              {{ if .Annotations.runbook_url }}*Runbook:* {{ .Annotations.runbook_url }}{{ end }}
              {{ end }}

      # Critical alerts - multi-channel notification
      - name: 'helm-critical-alerts'
        slack_configs:
          - api_url: '${SLACK_WEBHOOK_URL}'
            channel: '#helm-critical'
            color: 'danger'
            title: 'üö® CRITICAL: Helm Chart Specialist Alert'
            text: |
              {{ range .Alerts }}
              *CRITICAL ALERT*
              *Alert:* {{ .Annotations.summary }}
              *Component:* {{ .Labels.component }}
              *Environment:* {{ .Labels.environment }}
              *Description:* {{ .Annotations.description }}
              *Time:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
              {{ if .Annotations.runbook_url }}*Runbook:* {{ .Annotations.runbook_url }}{{ end }}
              {{ end }}

        pagerduty_configs:
          - routing_key: '${PAGERDUTY_INTEGRATION_KEY}'
            description: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.summary }}'
            severity: 'critical'
            details:
              component: '{{ .GroupLabels.component }}'
              environment: '{{ .GroupLabels.environment }}'
              firing_alerts: '{{ len .Alerts.Firing }}'

        email_configs:
          - to: 'oncall@company.com,platform-team@company.com'
            subject: 'üö® CRITICAL: {{ .GroupLabels.alertname }} in {{ .GroupLabels.environment }}'
            body: |
              Critical alert from Helm Chart Specialist:

              {{ range .Alerts }}
              Alert: {{ .Annotations.summary }}
              Component: {{ .Labels.component }}
              Environment: {{ .Labels.environment }}
              Description: {{ .Annotations.description }}
              Started: {{ .StartsAt.Format "2006-01-02 15:04:05 UTC" }}
              {{ if .Annotations.runbook_url }}Runbook: {{ .Annotations.runbook_url }}{{ end }}

              {{ end }}

      # Security incident response - immediate escalation
      - name: 'security-incident-response'
        slack_configs:
          - api_url: '${SLACK_WEBHOOK_URL}'
            channel: '#security-incidents'
            color: 'danger'
            title: 'üîí SECURITY INCIDENT: Helm Chart Specialist'
            text: |
              *SECURITY INCIDENT DETECTED*
              {{ range .Alerts }}
              *Alert:* {{ .Annotations.summary }}
              *Chart:* {{ .Labels.chart_name }}
              *Severity:* {{ .Labels.severity }}
              *Vulnerabilities:* {{ .Labels.vulnerability_count }}
              *Action Required:* Immediate investigation
              {{ if .Annotations.runbook_url }}*Response Runbook:* {{ .Annotations.runbook_url }}{{ end }}
              {{ end }}

        pagerduty_configs:
          - routing_key: '${PAGERDUTY_SECURITY_KEY}'
            description: 'Security Incident: {{ .CommonAnnotations.summary }}'
            severity: 'critical'
            details:
              incident_type: 'security'
              component: '{{ .GroupLabels.component }}'
              vulnerability_count: '{{ .GroupLabels.vulnerability_count }}'

        webhook_configs:
          - url: 'http://helm-advanced-alerting:8080/api/alerts/security'
            send_resolved: true
            http_config:
              basic_auth:
                username: '${WEBHOOK_USERNAME}'
                password: '${WEBHOOK_PASSWORD}'

      # Deployment critical alerts
      - name: 'deployment-critical'
        slack_configs:
          - api_url: '${SLACK_WEBHOOK_URL}'
            channel: '#deployment-issues'
            color: 'danger'
            title: '‚ö†Ô∏è DEPLOYMENT FAILURE: {{ .GroupLabels.environment }}'
            text: |
              *Deployment Failure Detected*
              {{ range .Alerts }}
              *Chart:* {{ .Labels.chart_name }}
              *Environment:* {{ .Labels.environment }}
              *Operation:* {{ .Labels.operation }}
              *Failure Rate:* {{ .Annotations.failure_rate }}
              *Time:* {{ .StartsAt.Format "15:04:05" }}
              {{ end }}

        webhook_configs:
          - url: 'http://helm-advanced-alerting:8080/api/alerts/deployment'
            send_resolved: true

      # Warning alerts
      - name: 'helm-warnings'
        slack_configs:
          - api_url: '${SLACK_WEBHOOK_URL}'
            channel: '#helm-warnings'
            color: 'warning'
            title: '‚ö†Ô∏è Warning: Helm Chart Issue'
            text: |
              {{ range .Alerts }}
              *Warning:* {{ .Annotations.summary }}
              *Component:* {{ .Labels.component }}
              *Environment:* {{ .Labels.environment }}
              {{ if .Annotations.runbook_url }}*Runbook:* {{ .Annotations.runbook_url }}{{ end }}
              {{ end }}

      # Performance optimization alerts with automated response
      - name: 'performance-optimization'
        slack_configs:
          - api_url: '${SLACK_WEBHOOK_URL}'
            channel: '#performance-optimization'
            color: 'warning'
            title: 'üìä Performance Alert: Optimization Triggered'
            text: |
              *Performance Issue Detected - Automated Optimization Initiated*
              {{ range .Alerts }}
              *Metric:* {{ .Labels.metric_name }}
              *Current Value:* {{ .Annotations.current_value }}
              *Threshold:* {{ .Annotations.threshold }}
              *Auto-Remediation:* {{ .Annotations.auto_remediation }}
              {{ end }}

        webhook_configs:
          - url: 'http://helm-advanced-alerting:8080/api/alerts/performance'
            send_resolved: true
            http_config:
              basic_auth:
                username: '${WEBHOOK_USERNAME}'
                password: '${WEBHOOK_PASSWORD}'

      # Business metrics alerts
      - name: 'business-alerts'
        slack_configs:
          - api_url: '${SLACK_WEBHOOK_URL}'
            channel: '#business-metrics'
            color: 'good'
            title: 'üìà Business Metric Alert'
            text: |
              *Business Metric Threshold Reached*
              {{ range .Alerts }}
              *Metric:* {{ .Annotations.metric_name }}
              *Current:* {{ .Annotations.current_value }}
              *Target:* {{ .Annotations.target_value }}
              *Impact:* {{ .Annotations.business_impact }}
              {{ end }}

        email_configs:
          - to: 'management@company.com,product-team@company.com'
            subject: 'Business Metric Alert: {{ .GroupLabels.metric_name }}'
            body: |
              Business metric alert from Helm Chart Specialist:

              {{ range .Alerts }}
              Metric: {{ .Annotations.metric_name }}
              Current Value: {{ .Annotations.current_value }}
              Target Value: {{ .Annotations.target_value }}
              Business Impact: {{ .Annotations.business_impact }}
              Time: {{ .StartsAt.Format "2006-01-02 15:04:05 UTC" }}
              {{ end }}

      # Development environment alerts (reduced frequency)
      - name: 'dev-alerts'
        slack_configs:
          - api_url: '${SLACK_WEBHOOK_URL}'
            channel: '#dev-alerts'
            color: 'good'
            title: 'üõ†Ô∏è Development Alert'
            text: |
              {{ range .Alerts }}
              *Dev Alert:* {{ .Annotations.summary }}
              *Environment:* development
              *Note:* Development environment alert (reduced priority)
              {{ end }}

---
# AlertManager Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
  labels:
    app: alertmanager
spec:
  replicas: 2
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9093"
    spec:
      serviceAccountName: alertmanager
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        fsGroup: 65534
      containers:
      - name: alertmanager
        image: prom/alertmanager:v0.26.0
        args:
          - '--config.file=/etc/alertmanager/alertmanager.yml'
          - '--storage.path=/alertmanager'
          - '--data.retention=120h'
          - '--web.listen-address=0.0.0.0:9093'
          - '--web.external-url=http://alertmanager.monitoring.local'
          - '--cluster.listen-address=0.0.0.0:9094'
        ports:
        - containerPort: 9093
          name: http
        - containerPort: 9094
          name: cluster
        env:
        - name: SLACK_WEBHOOK_URL
          valueFrom:
            secretKeyRef:
              name: alertmanager-secrets
              key: slack-webhook-url
        - name: PAGERDUTY_INTEGRATION_KEY
          valueFrom:
            secretKeyRef:
              name: alertmanager-secrets
              key: pagerduty-integration-key
        - name: PAGERDUTY_SECURITY_KEY
          valueFrom:
            secretKeyRef:
              name: alertmanager-secrets
              key: pagerduty-security-key
        - name: SMTP_USERNAME
          valueFrom:
            secretKeyRef:
              name: alertmanager-secrets
              key: smtp-username
        - name: SMTP_PASSWORD
          valueFrom:
            secretKeyRef:
              name: alertmanager-secrets
              key: smtp-password
        - name: WEBHOOK_USERNAME
          valueFrom:
            secretKeyRef:
              name: alertmanager-secrets
              key: webhook-username
        - name: WEBHOOK_PASSWORD
          valueFrom:
            secretKeyRef:
              name: alertmanager-secrets
              key: webhook-password
        volumeMounts:
        - name: config
          mountPath: /etc/alertmanager
          readOnly: true
        - name: templates
          mountPath: /etc/alertmanager/templates
          readOnly: true
        - name: storage
          mountPath: /alertmanager
        resources:
          requests:
            memory: 256Mi
            cpu: 100m
          limits:
            memory: 512Mi
            cpu: 500m
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: 9093
          initialDelaySeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 9093
          initialDelaySeconds: 5
          timeoutSeconds: 10
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
      volumes:
      - name: config
        configMap:
          name: alertmanager-config
      - name: templates
        configMap:
          name: alertmanager-templates
      - name: storage
        persistentVolumeClaim:
          claimName: alertmanager-storage

---
# AlertManager Service
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: monitoring
  labels:
    app: alertmanager
spec:
  selector:
    app: alertmanager
  ports:
  - name: http
    port: 9093
    targetPort: 9093
  - name: cluster
    port: 9094
    targetPort: 9094
  type: ClusterIP

---
# AlertManager PVC
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: alertmanager-storage
  namespace: monitoring
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: gp3

---
# AlertManager ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: alertmanager
  namespace: monitoring

---
# AlertManager Secrets (template - replace with actual values)
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-secrets
  namespace: monitoring
type: Opaque
data:
  # Base64 encoded values - replace with actual credentials
  slack-webhook-url: aHR0cHM6Ly9ob29rcy5zbGFjay5jb20vLi4u  # https://hooks.slack.com/...
  pagerduty-integration-key: cGFnZXJkdXR5LWludGVncmF0aW9uLWtleQ==  # pagerduty-integration-key
  pagerduty-security-key: cGFnZXJkdXR5LXNlY3VyaXR5LWtleQ==      # pagerduty-security-key
  smtp-username: c210cC11c2VybmFtZQ==                          # smtp-username
  smtp-password: c210cC1wYXNzd29yZA==                          # smtp-password
  webhook-username: d2ViaG9vay11c2VybmFtZQ==                    # webhook-username
  webhook-password: d2ViaG9vay1wYXNzd29yZA==                    # webhook-password

---
# AlertManager Templates ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-templates
  namespace: monitoring
data:
  helm-alerts.tmpl: |
    {{ define "helm.title" }}
    [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.SortedPairs.Values | join " " }}
    {{ end }}

    {{ define "helm.color" }}
    {{ if eq .Status "firing" }}
    {{ if eq .CommonLabels.severity "critical" }}danger{{ else if eq .CommonLabels.severity "warning" }}warning{{ else }}good{{ end }}
    {{ else }}good{{ end }}
    {{ end }}

    {{ define "helm.text" }}
    {{ range .Alerts }}
    {{ if .Annotations.summary }}*Summary:* {{ .Annotations.summary }}{{ end }}
    {{ if .Labels.component }}*Component:* {{ .Labels.component }}{{ end }}
    {{ if .Labels.environment }}*Environment:* {{ .Labels.environment }}{{ end }}
    {{ if .Annotations.description }}*Description:* {{ .Annotations.description }}{{ end }}
    {{ if .Annotations.runbook_url }}*Runbook:* {{ .Annotations.runbook_url }}{{ end }}
    *Started:* {{ .StartsAt.Format "2006-01-02 15:04:05 UTC" }}
    {{ if .EndsAt }}*Ended:* {{ .EndsAt.Format "2006-01-02 15:04:05 UTC" }}{{ end }}
    ---
    {{ end }}
    {{ end }}